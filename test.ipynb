{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c7e4c041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -U langchain langchain-community faiss-cpu langchain-ollama openai tiktoken\n",
    "\n",
    "from langchain_ollama.embeddings import OllamaEmbeddings\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores.faiss import FAISS\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents.stuff import create_stuff_documents_chain\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import Document\n",
    "import csv, json\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366e99a8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Could not import chromadb python package. Please install it with `pip install chromadb`.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.3/envs/doc_analyze/lib/python3.12/site-packages/langchain_community/vectorstores/chroma.py:83\u001b[39m, in \u001b[36mChroma.__init__\u001b[39m\u001b[34m(self, collection_name, embedding_function, persist_directory, client_settings, collection_metadata, client, relevance_score_fn)\u001b[39m\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mchromadb\u001b[39;00m\n\u001b[32m     84\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mchromadb\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'chromadb'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     21\u001b[39m chunks = [LC_Document(page_content=chunk) \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m list_of_chunk_texts]\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# 3. Create a Chroma vector store with Ollama embeddings\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m vectordb = \u001b[43mChroma\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m base_retriever = vectordb.as_retriever(search_kwargs={\u001b[33m\"\u001b[39m\u001b[33mk\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m5\u001b[39m})\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# 4. Add an optional LLM-based compressor for more focused context\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.3/envs/doc_analyze/lib/python3.12/site-packages/langchain_community/vectorstores/chroma.py:887\u001b[39m, in \u001b[36mChroma.from_documents\u001b[39m\u001b[34m(cls, documents, embedding, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[39m\n\u001b[32m    885\u001b[39m texts = [doc.page_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[32m    886\u001b[39m metadatas = [doc.metadata \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[32m--> \u001b[39m\u001b[32m887\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfrom_texts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    889\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m=\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    890\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    891\u001b[39m \u001b[43m    \u001b[49m\u001b[43mids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    892\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    893\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    894\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclient_settings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclient_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    895\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    896\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcollection_metadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollection_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    897\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    898\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.3/envs/doc_analyze/lib/python3.12/site-packages/langchain_community/vectorstores/chroma.py:817\u001b[39m, in \u001b[36mChroma.from_texts\u001b[39m\u001b[34m(cls, texts, embedding, metadatas, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[39m\n\u001b[32m    784\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m    785\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfrom_texts\u001b[39m(\n\u001b[32m    786\u001b[39m     \u001b[38;5;28mcls\u001b[39m: Type[Chroma],\n\u001b[32m   (...)\u001b[39m\u001b[32m    796\u001b[39m     **kwargs: Any,\n\u001b[32m    797\u001b[39m ) -> Chroma:\n\u001b[32m    798\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Create a Chroma vectorstore from a raw documents.\u001b[39;00m\n\u001b[32m    799\u001b[39m \n\u001b[32m    800\u001b[39m \u001b[33;03m    If a persist_directory is specified, the collection will be persisted there.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    815\u001b[39m \u001b[33;03m        Chroma: Chroma vectorstore.\u001b[39;00m\n\u001b[32m    816\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m817\u001b[39m     chroma_collection = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    818\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    819\u001b[39m \u001b[43m        \u001b[49m\u001b[43membedding_function\u001b[49m\u001b[43m=\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    820\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    821\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclient_settings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclient_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    822\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    823\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcollection_metadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollection_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    824\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    826\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    827\u001b[39m         ids = [\u001b[38;5;28mstr\u001b[39m(uuid.uuid4()) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m texts]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.3/envs/doc_analyze/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:222\u001b[39m, in \u001b[36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    220\u001b[39m     warned = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    221\u001b[39m     emit_warning()\n\u001b[32m--> \u001b[39m\u001b[32m222\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.3/envs/doc_analyze/lib/python3.12/site-packages/langchain_community/vectorstores/chroma.py:86\u001b[39m, in \u001b[36mChroma.__init__\u001b[39m\u001b[34m(self, collection_name, embedding_function, persist_directory, client_settings, collection_metadata, client, relevance_score_fn)\u001b[39m\n\u001b[32m     84\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mchromadb\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\n\u001b[32m     85\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m     87\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCould not import chromadb python package. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     88\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease install it with `pip install chromadb`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     89\u001b[39m     )\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m client \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     92\u001b[39m     \u001b[38;5;28mself\u001b[39m._client_settings = client_settings\n",
      "\u001b[31mImportError\u001b[39m: Could not import chromadb python package. Please install it with `pip install chromadb`."
     ]
    }
   ],
   "source": [
    "# 0Ô∏è‚É£ Ensure OPENAI_API_KEY is set in environment\n",
    "os.environ.setdefault(\"OPENAI_API_KEY\", \"sk-...\")\n",
    "\n",
    "# ‚úÖ 1. Load controls from CSV\n",
    "def load_controls_from_csv(path=\"ISO_27001_2022_Controls_List.csv\"):\n",
    "    with open(path, newline='', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        return [{\"id\": r[\"Control ID\"].strip(), \"title\": r[\"Control Title\"].strip()} for r in reader]\n",
    "\n",
    "controls = load_controls_from_csv()\n",
    "groups = [controls[i:i+20] for i in range(0, len(controls), 20)]\n",
    "\n",
    "# üîÑ 2. Load PDF and chunk text\n",
    "loader = PyPDFLoader(\"your_doc.pdf\")\n",
    "docs = loader.load()\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=150, length_function=len)\n",
    "chunks = splitter.split_documents(docs)\n",
    "\n",
    "# üí° 3. Build FAISS vector store with Ollama embeddings\n",
    "embeddings = OllamaEmbeddings(model=\"llama3\")\n",
    "faiss_store = FAISS.from_documents(chunks, embeddings)  # uses FAISS index under the hood :contentReference[oaicite:1]{index=1}\n",
    "base_retriever = faiss_store.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "# ‚öôÔ∏è 4. Add LLM-based contextual compression\n",
    "compressor = LLMChainExtractor.from_llm(ChatOpenAI(model_name=\"gpt-4.1-mini\", temperature=0))\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_retriever=base_retriever,\n",
    "    base_compressor=compressor\n",
    ")\n",
    "\n",
    "# üß† 5. Build RetrievalQA chain for grouped control audit\n",
    "llm = ChatOpenAI(model_name=\"gpt-4.1-mini\", temperature=0)\n",
    "system_prompt = \"You are an ISO‚ÄØ27001 auditor. Answer in JSON using the context and provided controls.\"\n",
    "human_template = \"\"\"\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Please audit these controls:\n",
    "{controls_list}\n",
    "\n",
    "Return JSON:\n",
    "[\n",
    "  {{\n",
    "    \"Control ID\": \"...\",\n",
    "    \"Control Title\": \"...\",\n",
    "    \"Compliance\": \"‚úîÔ∏è or ‚ùå\",\n",
    "    \"Notes / Gaps\": \"...\",\n",
    "    \"Suggested Implementation\": \"...\"\n",
    "  }},\n",
    "  ‚Ä¶\n",
    "]\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"human\", human_template),\n",
    "])\n",
    "combine_chain = create_stuff_documents_chain(llm=llm, prompt=prompt)\n",
    "qa_chain = create_retrieval_chain(\n",
    "    retriever=compression_retriever,\n",
    "    combine_documents_chain=combine_chain\n",
    ")\n",
    "\n",
    "# 6. Execute audits group-by-group\n",
    "all_results = []\n",
    "for group in groups:\n",
    "    controls_list = \"\\n\".join(f\"{c['id']} - {c['title']}\" for c in group)\n",
    "    result = qa_chain.invoke({\"input\": controls_list})\n",
    "    parsed = json.loads(result)\n",
    "    all_results.extend(parsed)\n",
    "\n",
    "# 7. Save output\n",
    "with open(\"audit.json\", \"w\") as f:\n",
    "    json.dump(all_results, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Completed audit: {len(all_results)} controls evaluated\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30f9cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 5. Build the RetrievalQA chain\n",
    "llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0.3, api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "system_prompt = \"You are an ISO 27001 auditor. Use context to audit controls and output JSON.\"\n",
    "human_prompt = \"\"\"\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Audit these controls:\n",
    "{controls_list}\n",
    "\n",
    "Return a JSON array:\n",
    "[\n",
    "  {\n",
    "    \"Control ID\": \"...\",\n",
    "    \"Control Title\": \"...\",\n",
    "    \"Compliance\": \"‚úîÔ∏è or ‚ùå\",\n",
    "    \"Notes / Gaps\": \"...\",\n",
    "    \"Suggested Implementation\": \"...\"\n",
    "  },\n",
    "  ...\n",
    "]\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"human\", human_prompt),\n",
    "])\n",
    "combine_chain = create_stuff_documents_chain(llm=llm, prompt=prompt)\n",
    "qa_chain = create_retrieval_chain(\n",
    "    retriever=compression_retriever,\n",
    "    combine_documents_chain=combine_chain\n",
    ")\n",
    "\n",
    "# 6. Load the control list and batch them\n",
    "def load_controls_from_csv(path=\"iso27001_annexA_controls.csv\"):\n",
    "    with open(path, newline='', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        # Optionally handle types, strip spaces, etc.\n",
    "        controls = [\n",
    "            {\n",
    "                \"id\": row[\"Control ID\"].strip(),\n",
    "                \"title\": row[\"Control Title\"].strip()\n",
    "            }\n",
    "            for row in reader\n",
    "        ]\n",
    "    return controls\n",
    "\n",
    "controls = load_controls_from_csv()\n",
    "groups = [controls[i:i+20] for i in range(0, len(controls), 20)]\n",
    "\n",
    "# 7. Execute the audit\n",
    "all_results = []\n",
    "for group in groups:\n",
    "    controls_list = \"\\n\".join(f\"{c['id']} - {c['title']}\" for c in group)\n",
    "    result = qa_chain.invoke({\"input\": controls_list})\n",
    "    parsed = json.loads(result)\n",
    "    all_results.extend(parsed)\n",
    "\n",
    "# 8. Save output\n",
    "with open(\"audit.json\", \"w\") as f:\n",
    "    json.dump(all_results, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Completed audit: {len(all_results)} controls evaluated\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "doc_analyze",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
